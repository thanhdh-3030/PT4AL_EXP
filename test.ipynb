{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import Loader, Loader2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "import numpy as np\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "latent_dim=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plabels_by_vae(vae, discriminator,samples,cycle):\n",
    "    # sample1k = []\n",
    "    sub5k = Loader2(is_train=False,  transform=transform_test, path_list=samples)\n",
    "    ploader = torch.utils.data.DataLoader(sub5k, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "    unlabel_scores = []\n",
    "    vae.eval()\n",
    "    discriminator.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, targets) in enumerate(ploader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            _, _, mu, _ = vae(inputs)\n",
    "            preds = discriminator(mu)\n",
    "            # preds = preds.cpu().data\n",
    "            unlabel_scores.append(preds.view(preds.size(0)).item())\n",
    "            # progress_bar(idx, len(ploader))\n",
    "    idx=np.argsort(unlabel_scores)\n",
    "    samples=np.array(samples)\n",
    "    return samples[idx[-1000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = model.VAE(latent_dim)\n",
    "discriminator = model.Discriminator(latent_dim)\n",
    "vae=vae.to(device)\n",
    "discriminator=discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./loss/batch_{1}.txt', 'r') as f:\n",
    "    samples = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub5k = Loader2(is_train=False,  transform=transform_test, path_list=samples)\n",
    "ploader = torch.utils.data.DataLoader(sub5k, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "unlabel_scores = []\n",
    "vae.eval()\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(ploader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        _, _, mu, _ = vae(inputs)\n",
    "        preds = discriminator(mu)\n",
    "        # preds = preds.cpu().data\n",
    "        print(preds)\n",
    "        unlabel_scores.append(preds.view(preds.size(0)).item())\n",
    "        # progress_bar(idx, len(ploader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.argsort(unlabel_scores)\n",
    "samples=np.array(samples)\n",
    "samples[idx[-1000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(unlabel_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(torch.from_numpy(np.array(unlabel_scores)),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(unlabel_scores)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1k=get_plabels_by_vae(vae, discriminator,samples,cycle=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityspace_dir='/home/dang.hong.thanh/datasets/Cityspaces'\n",
    "# image_transform=transforms.Compose([\n",
    "#     # you can add other transformations in this list\n",
    "#     transforms.ToTensor()])\n",
    "# target_transform = transforms.Compose([\n",
    "#     transforms.PILToTensor()\n",
    "# ])\n",
    "interp = nn.Upsample(size=(50, 50), mode='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calucate uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./loss_seg/batch_0.txt', 'r') as f:\n",
    "            samples = f.readlines()\n",
    "samples=[sample[:-1] for sample in samples] # remove newline character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pth='/home/dang.hong.thanh/PT4AL/checkpoint/ckpt.pth'\n",
    "pth='/home/dang.hong.thanh/PT4AL/checkpoint_seg/main_0.pth'\n",
    "checkpoint=torch.load(pth)\n",
    "net.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    resize = 512\n",
    "    base_size = 512\n",
    "    crop_size = 50\n",
    "    multi_scale= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_seg_plabels_confidence(net, samples, cycle):\n",
    "    # dictionary with 10 keys as class labels\n",
    "    # class_dict = {}\n",
    "    # [class_dict.setdefault(x,[]) for x in range(10)]\n",
    "device='cuda'\n",
    "sample1k = []\n",
    "subset = ActiveCityscapesSegmentation(args,cityspace_dir,split='train',files=samples)\n",
    "subset=CityscapesSegmentation(args,cityspace_dir,split='val')\n",
    "ploader = torch.utils.data.DataLoader(subset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "uncertainty_scores = []\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, samples in enumerate(ploader):\n",
    "        inputs,targets=samples['image'],samples['label']\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        outputs=interp(outputs)\n",
    "        # scores, predicted = outputs.max(1)\n",
    "        # save top1 confidence score \n",
    "        # outputs = F.normalize(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        # top1_scores.append(probs[0][predicted.item()])\n",
    "        # top1_scores.append(probs[0][predicted.item()].cpu().item())\n",
    "        # max_probs,indices=torch.max(probs,dim=1)\n",
    "        # # print(outputs.size())\n",
    "        # print(max_probs)\n",
    "        # print(indices)\n",
    "        # measures=torch.logical_and(probs>=0.35,probs<=0.75)\n",
    "        best_probs,_=torch.max(probs,dim=1)\n",
    "        measures=torch.lt(best_probs,0.5)    \n",
    "        uncertainty_score=torch.sum(measures,dim=(-1,-2))\n",
    "        # print(uncertainty_score)\n",
    "        uncertainty_scores.append(uncertainty_score.cpu().item())\n",
    "        # progress_bar(idx, len(ploader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=subset[0]\n",
    "inputs,targets=samples['image'],samples['label']\n",
    "inputs, targets = inputs.to(device), targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(inputs.unsqueeze(0))\n",
    "outputs=interp(outputs)\n",
    "scores, predicted = outputs.max(1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(outputs, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = F.normalize(outputs, dim=1)\n",
    "scores, predicted = n_outputs.max(1)\n",
    "print(scores)\n",
    "probs = F.softmax(n_outputs, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs,indices=torch.max(probs,dim=1)\n",
    "# print(outputs.size())\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs,indices=torch.topk(probs,k=2,dim=1)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_logits, _=torch.topk(outputs,dim=1,k=2)\n",
    "print(max_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(outputs, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs,indices=torch.max(probs,dim=1)\n",
    "# print(outputs.size())\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures=torch.lt(max_probs,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(measures,(-1,-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "238/(50*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_score=torch.sum(measures,(-1,-2))\n",
    "uncertainty_scores.append(uncertainty_score.cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.core.evaluation.metrics import eval_metrics\n",
    "results = []\n",
    "gt_seg_maps=[]\n",
    "model.eval()\n",
    "device='cuda'\n",
    "with torch.no_grad():\n",
    "    for batch_idx, samples in enumerate(testloader):\n",
    "        # inputs,targets=samples['image'],samples['label']\n",
    "        # inputs,targets=samples['img'].data[0],samples['gt_semantic_seg'].data[0]\n",
    "        inputs,targets=samples['img'][0],samples['gt_semantic_seg'][0]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # data=dict()\n",
    "        # data['img']=samples['img'].data\n",
    "        # # data['gt_semantic_seg']=samples['gt_semantic_seg'].data\n",
    "        # data['img_metas']=samples['img_metas'].data\n",
    "        # outputs=model(return_loss=False,**samples)\n",
    "        # print(outputs.size())\n",
    "        # outputs=val_interp(outputs)\n",
    "        # (_,H,W)=targets.size().data\n",
    "        # outputs=nn.Upsample(size=(targets.size()[-2],targets.size()[-1]), mode='bilinear', align_corners=True)(outputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        # print(predicted.size())\n",
    "        # print(targets[0].size())\n",
    "        results.append(predicted.squeeze(0).cpu().numpy())\n",
    "        gt_seg_maps.append(targets[0].squeeze(0).cpu().numpy())\n",
    "ret_metrics=eval_metrics(results,gt_seg_maps,num_classes=19,ignore_index=225,metrics=['mIoU','mDice'],reduce_zero_label=False)\n",
    "# print(ret_metrics['Dice'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6828102191699298\n",
      "0.8041502137207482\n"
     ]
    }
   ],
   "source": [
    "print(ret_metrics['IoU'].mean())\n",
    "print(ret_metrics['Dice'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate by mmseg data and mmseg model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'CityscapesDataset'\n",
    "data_root='/home/dang.hong.thanh/datasets/Cityspaces'\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "# test_pipeline = [\n",
    "#     dict(type='LoadImageFromFile'),\n",
    "#     dict(\n",
    "#         type='MultiScaleFlipAug',\n",
    "#         img_scale=(2048, 1024),\n",
    "#         # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
    "#         flip=False,\n",
    "#         transforms=[\n",
    "#             dict(type='Resize', keep_ratio=True),\n",
    "#             dict(type='RandomFlip'),\n",
    "#             dict(type='Normalize', **img_norm_cfg),\n",
    "#             dict(type='ImageToTensor', keys=['img']),\n",
    "#             dict(type='Collect', keys=['img']),\n",
    "#         ])\n",
    "# ]\n",
    "# crop_size = (512, 1024)\n",
    "# train_pipeline = [\n",
    "#     dict(type='LoadImageFromFile'),\n",
    "#     dict(type='LoadAnnotations'),\n",
    "#     dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n",
    "#     dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),\n",
    "#     dict(type='RandomFlip', prob=0.5),\n",
    "#     dict(type='PhotoMetricDistortion'),\n",
    "#     dict(type='Normalize', **img_norm_cfg),\n",
    "#     dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\n",
    "#     dict(type='DefaultFormatBundle'),\n",
    "#     dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n",
    "# ]\n",
    "val_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(2048, 1024),\n",
    "        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='RandomFlip'),\n",
    "            dict(type='Normalize', **img_norm_cfg),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img','gt_semantic_seg']),\n",
    "        ]),\n",
    "        # dict(type='Collect', keys=['img','gt_semantic_seg'])\n",
    "]\n",
    "val_cfg=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        img_dir='images/val',\n",
    "        ann_dir='gtFine/val',\n",
    "        pipeline=val_pipeline,\n",
    "        img_suffix='',\n",
    "        seg_map_suffix='',\n",
    "        split='/home/dang.hong.thanh/PT4AL/loss_seg/batch_0.txt',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 05:20:34,585 - mmseg - INFO - Loaded 297 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([3, 1024, 2048])\n",
      "(1024, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "from mmseg.datasets import build_dataset\n",
    "val_dataset=build_dataset(val_cfg)\n",
    "print(val_dataset[0]['img'][0].size())\n",
    "print(val_dataset[0]['gt_semantic_seg'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.parallel import collate\n",
    "from functools import partial\n",
    "import torch\n",
    "testloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2,drop_last=False,\n",
    "    collate_fn=partial(collate, samples_per_gpu=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buld model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: pspnet_r18-d8_512x1024_80k_cityscapes_20201225_021458-09ffa746.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dang.hong.thanh/PT4AL/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mmseg.apis import init_segmentor\n",
    "config_file = '/home/dang.hong.thanh/mmsegmentation/configs/pspnet/pspnet_r18-d8_512x1024_80k_cityscapes.py'\n",
    "checkpoint_file = 'pspnet_r18-d8_512x1024_80k_cityscapes_20201225_021458-09ffa746.pth'\n",
    "model =init_segmentor(config_file, checkpoint_file, device='cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesing: 500it [00:24, 20.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from mmseg.core.evaluation.metrics import intersect_and_union\n",
    "from mmcv.parallel import DataContainer\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "device='cuda'\n",
    "pre_eval_results = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in tqdm(enumerate(testloader),ncols = 100,\n",
    "               desc =\"Tesing\"):\n",
    "        # inputs,targets=samples['image'],samples['label']\n",
    "        # inputs,targets=samples['img'].data[0],samples['gt_semantic_seg'].data[0]\n",
    "        # targets=data['gt_semantic_seg'][0]\n",
    "        # targets = targets.to(device)\n",
    "        input_data=dict()\n",
    "        input_data['img_metas']=data['img_metas'][0].data\n",
    "        input_data['img']=[data['img'][0].data.to(device)]\n",
    "        outputs = model(return_loss=False, **input_data)\n",
    "        # outputs=val_interp(outputs)\n",
    "        # (_,H,W)=targets.size().data\n",
    "        # outputs=nn.Upsample(size=(targets.size()[-2],targets.size()[-1]), mode='bilinear', align_corners=True)(outputs)\n",
    "        # _, predicted = outputs[0].max(1)\n",
    "        pred=outputs[0]\n",
    "        seg_map=data['gt_semantic_seg'][0].cpu().squeeze().numpy()\n",
    "        pre_eval_result=intersect_and_union(pred,seg_map,num_classes=19,ignore_index=255,label_map=dict(),reduce_zero_label=False)\n",
    "        pre_eval_results.append(pre_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 2048])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gt_semantic_seg'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74865\n"
     ]
    }
   ],
   "source": [
    "from mmseg.core import pre_eval_to_metrics\n",
    "ret_metrics=pre_eval_to_metrics(pre_eval_results,metrics=['mIoU'])\n",
    "print(ret_metrics['IoU'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefind model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "norm_cfg = dict(type='BN', requires_grad=True)\n",
    "model_cfg = dict(\n",
    "    type='SunSegmentor',\n",
    "    backbone=dict(\n",
    "        type='ResNetV1c',\n",
    "        pretrained='open-mmlab://resnet18_v1c',\n",
    "        depth=18,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        dilations=(1, 1, 2, 4),\n",
    "        strides=(1, 2, 1, 1),\n",
    "        norm_cfg=norm_cfg,\n",
    "        norm_eval=False,\n",
    "        style='pytorch',\n",
    "        contract_dilation=True),\n",
    "    decode_head=dict(\n",
    "        type='PSPHead',\n",
    "        in_channels=512,\n",
    "        in_index=3,\n",
    "        channels=128,\n",
    "        pool_scales=(1, 2, 3, 6),\n",
    "        dropout_ratio=0.1,\n",
    "        num_classes=19,\n",
    "        norm_cfg=norm_cfg,\n",
    "        align_corners=False,\n",
    "        loss_decode=dict(\n",
    "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n",
    "    # auxiliary_head=dict(\n",
    "    #     type='FCNHead',\n",
    "    #     in_channels=256,\n",
    "    #     in_index=2,\n",
    "    #     channels=64,\n",
    "    #     num_convs=1,\n",
    "    #     concat_input=False,\n",
    "    #     dropout_ratio=0.1,\n",
    "    #     num_classes=19,\n",
    "    #     norm_cfg=norm_cfg,\n",
    "    #     align_corners=True,\n",
    "    #     loss_decode=dict(\n",
    "    #         type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),\n",
    "    # model training and testing settings\n",
    "    train_cfg=dict(),\n",
    "    # test_cfg=dict(mode='slide', crop_size=(769, 769), stride=(513, 513))\n",
    "    # test_cfg=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dang.hong.thanh/PT4AL/mmseg/models/backbones/resnet.py:431: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n",
      "/home/dang.hong.thanh/PT4AL/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n",
      "2023-02-24 01:45:36,244 - mmseg - INFO - initialize ResNetV1c with init_cfg {'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet18_v1c'}\n",
      "2023-02-24 01:45:36,244 - mmcv - INFO - load model from: open-mmlab://resnet18_v1c\n",
      "2023-02-24 01:45:36,244 - mmcv - INFO - load checkpoint from openmmlab path: open-mmlab://resnet18_v1c\n",
      "2023-02-24 01:45:36,256 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "2023-02-24 01:45:36,259 - mmseg - INFO - initialize PSPHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
      "2023-02-24 01:45:36,265 - mmseg - INFO - \n",
      "backbone.stem.0.weight - torch.Size([32, 3, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,265 - mmseg - INFO - \n",
      "backbone.stem.1.weight - torch.Size([32]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,265 - mmseg - INFO - \n",
      "backbone.stem.1.bias - torch.Size([32]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,265 - mmseg - INFO - \n",
      "backbone.stem.3.weight - torch.Size([32, 32, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,265 - mmseg - INFO - \n",
      "backbone.stem.4.weight - torch.Size([32]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,266 - mmseg - INFO - \n",
      "backbone.stem.4.bias - torch.Size([32]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,266 - mmseg - INFO - \n",
      "backbone.stem.6.weight - torch.Size([64, 32, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,266 - mmseg - INFO - \n",
      "backbone.stem.7.weight - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,266 - mmseg - INFO - \n",
      "backbone.stem.7.bias - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,266 - mmseg - INFO - \n",
      "backbone.layer1.0.conv1.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,266 - mmseg - INFO - \n",
      "backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,267 - mmseg - INFO - \n",
      "backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,267 - mmseg - INFO - \n",
      "backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,267 - mmseg - INFO - \n",
      "backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,267 - mmseg - INFO - \n",
      "backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,267 - mmseg - INFO - \n",
      "backbone.layer1.1.conv1.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,267 - mmseg - INFO - \n",
      "backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,268 - mmseg - INFO - \n",
      "backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,268 - mmseg - INFO - \n",
      "backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,268 - mmseg - INFO - \n",
      "backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,268 - mmseg - INFO - \n",
      "backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,269 - mmseg - INFO - \n",
      "backbone.layer2.0.conv1.weight - torch.Size([128, 64, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,270 - mmseg - INFO - \n",
      "backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,270 - mmseg - INFO - \n",
      "backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,270 - mmseg - INFO - \n",
      "backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,270 - mmseg - INFO - \n",
      "backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,270 - mmseg - INFO - \n",
      "backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,271 - mmseg - INFO - \n",
      "backbone.layer2.0.downsample.0.weight - torch.Size([128, 64, 1, 1]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,271 - mmseg - INFO - \n",
      "backbone.layer2.0.downsample.1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,271 - mmseg - INFO - \n",
      "backbone.layer2.0.downsample.1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,271 - mmseg - INFO - \n",
      "backbone.layer2.1.conv1.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,272 - mmseg - INFO - \n",
      "backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,272 - mmseg - INFO - \n",
      "backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,272 - mmseg - INFO - \n",
      "backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,272 - mmseg - INFO - \n",
      "backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,273 - mmseg - INFO - \n",
      "backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,273 - mmseg - INFO - \n",
      "backbone.layer3.0.conv1.weight - torch.Size([256, 128, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,273 - mmseg - INFO - \n",
      "backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,273 - mmseg - INFO - \n",
      "backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,274 - mmseg - INFO - \n",
      "backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,274 - mmseg - INFO - \n",
      "backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,274 - mmseg - INFO - \n",
      "backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,274 - mmseg - INFO - \n",
      "backbone.layer3.0.downsample.0.weight - torch.Size([256, 128, 1, 1]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,274 - mmseg - INFO - \n",
      "backbone.layer3.0.downsample.1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,274 - mmseg - INFO - \n",
      "backbone.layer3.0.downsample.1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,275 - mmseg - INFO - \n",
      "backbone.layer3.1.conv1.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,275 - mmseg - INFO - \n",
      "backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,275 - mmseg - INFO - \n",
      "backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,276 - mmseg - INFO - \n",
      "backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,276 - mmseg - INFO - \n",
      "backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,276 - mmseg - INFO - \n",
      "backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,276 - mmseg - INFO - \n",
      "backbone.layer4.0.conv1.weight - torch.Size([512, 256, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,276 - mmseg - INFO - \n",
      "backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,276 - mmseg - INFO - \n",
      "backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,277 - mmseg - INFO - \n",
      "backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,277 - mmseg - INFO - \n",
      "backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,277 - mmseg - INFO - \n",
      "backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,277 - mmseg - INFO - \n",
      "backbone.layer4.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,278 - mmseg - INFO - \n",
      "backbone.layer4.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,278 - mmseg - INFO - \n",
      "backbone.layer4.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,278 - mmseg - INFO - \n",
      "backbone.layer4.1.conv1.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,278 - mmseg - INFO - \n",
      "backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,278 - mmseg - INFO - \n",
      "backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,279 - mmseg - INFO - \n",
      "backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,279 - mmseg - INFO - \n",
      "backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,279 - mmseg - INFO - \n",
      "backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from open-mmlab://resnet18_v1c \n",
      " \n",
      "2023-02-24 01:45:36,279 - mmseg - INFO - \n",
      "decode_head.conv_seg.weight - torch.Size([19, 128, 1, 1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2023-02-24 01:45:36,279 - mmseg - INFO - \n",
      "decode_head.conv_seg.bias - torch.Size([19]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.0.1.conv.weight - torch.Size([128, 512, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.0.1.bn.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.0.1.bn.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.1.1.conv.weight - torch.Size([128, 512, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.1.1.bn.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.1.1.bn.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.2.1.conv.weight - torch.Size([128, 512, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.2.1.bn.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,280 - mmseg - INFO - \n",
      "decode_head.psp_modules.2.1.bn.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,281 - mmseg - INFO - \n",
      "decode_head.psp_modules.3.1.conv.weight - torch.Size([128, 512, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,281 - mmseg - INFO - \n",
      "decode_head.psp_modules.3.1.bn.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,281 - mmseg - INFO - \n",
      "decode_head.psp_modules.3.1.bn.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,281 - mmseg - INFO - \n",
      "decode_head.bottleneck.conv.weight - torch.Size([128, 1024, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in ConvModule  \n",
      " \n",
      "2023-02-24 01:45:36,281 - mmseg - INFO - \n",
      "decode_head.bottleneck.bn.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n",
      "2023-02-24 01:45:36,281 - mmseg - INFO - \n",
      "decode_head.bottleneck.bn.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SunSegmentor  \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmseg.models.builder import build_segmentor\n",
    "model = build_segmentor(model_cfg)\n",
    "model.init_weights()\n",
    "model = model.to(device)\n",
    "import torch\n",
    "# checkpoint=torch.load('/home/dang.hong.thanh/PT4AL/pspnet_r18-d8_512x1024_80k_cityscapes_20201225_021458-09ffa746.pth')\n",
    "# model.load_state_dict(checkpoint['state_dict'],strict=True)\n",
    "checkpoint1=torch.load('/home/dang.hong.thanh/PT4AL/checkpoint/ckpt.pth')\n",
    "model.load_state_dict(checkpoint1['net'],strict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesing: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, samples \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(testloader),ncols \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[1;32m      8\u001b[0m                desc \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTesing\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m         \u001b[39m# inputs,targets=samples['image'],samples['label']\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[39m# inputs,targets=samples['img'].data[0],samples['gt_semantic_seg'].data[0]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         inputs,targets\u001b[39m=\u001b[39msamples[\u001b[39m'\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39m],samples[\u001b[39m'\u001b[39m\u001b[39mgt_semantic_seg\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m         inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'img'"
     ]
    }
   ],
   "source": [
    "from mmseg.core.evaluation.metrics import intersect_and_union\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "device='cuda'\n",
    "pre_eval_results = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, samples in tqdm(enumerate(testloader),ncols = 100,\n",
    "               desc =\"Tesing\"):\n",
    "        # inputs,targets=samples['image'],samples['label']\n",
    "        # inputs,targets=samples['img'].data[0],samples['gt_semantic_seg'].data[0]\n",
    "        inputs,targets=samples['img'][0],samples['gt_semantic_seg'][0]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # outputs=val_interp(outputs)\n",
    "        # (_,H,W)=targets.size().data\n",
    "        # outputs=nn.Upsample(size=(targets.size()[-2],targets.size()[-1]), mode='bilinear', align_corners=True)(outputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        pred=predicted.squeeze(0).cpu().numpy()\n",
    "        seg_map=targets.squeeze(0).cpu().numpy()\n",
    "        pre_eval_result=intersect_and_union(pred,seg_map,num_classes=19,ignore_index=255,label_map=dict(),reduce_zero_label=False)\n",
    "        pre_eval_results.append(pre_eval_result)\n",
    "\n",
    "\n",
    "from mmseg.core import pre_eval_to_metrics\n",
    "ret_metrics=pre_eval_to_metrics(pre_eval_results,metrics=['mIoU'])\n",
    "print(ret_metrics['IoU'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 val images\n"
     ]
    }
   ],
   "source": [
    "CITYSCAPES_DIR='/home/dang.hong.thanh/datasets/Cityspaces'\n",
    "class Args:\n",
    "    a='a'\n",
    "args=Args\n",
    "args.resize = 512\n",
    "args.base_size = 512\n",
    "# args.crop_size = 321\n",
    "args.crop_size = 512\n",
    "args.batch_size=1\n",
    "args.resume=False\n",
    "from dataloaders.datasets.cityscapes import CityscapesSegmentation\n",
    "import torch\n",
    "val_dataset=CityscapesSegmentation(args,root=CITYSCAPES_DIR,split='val')\n",
    "testloader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2,drop_last=False)\n",
    "# import torch.nn as nn\n",
    "# val_interp = nn.Upsample(size=(args.crop_size, args.crop_size*2), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tesing: 500it [00:24, 20.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from mmseg.core.evaluation.metrics import intersect_and_union\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "device='cuda'\n",
    "pre_eval_results = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, samples in tqdm(enumerate(testloader),ncols = 100,\n",
    "               desc =\"Tesing\"):\n",
    "        inputs,targets=samples['image'],samples['label']\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.size())\n",
    "        # outputs=val_interp(outputs)\n",
    "        # _,H,W=targets.size()\n",
    "        # outputs=nn.Upsample(size=(H,W), mode='bilinear', align_corners=True)(outputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        pred=predicted.squeeze(0).cpu().numpy()\n",
    "        seg_map=targets.squeeze(0).cpu().numpy()\n",
    "        pre_eval_result=intersect_and_union(pred,seg_map,num_classes=19,ignore_index=255,label_map=dict(),reduce_zero_label=False)\n",
    "        pre_eval_results.append(pre_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47525832\n"
     ]
    }
   ],
   "source": [
    "from mmseg.core import pre_eval_to_metrics\n",
    "ret_metrics=pre_eval_to_metrics(pre_eval_results,metrics=['mIoU'])\n",
    "print(ret_metrics['IoU'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0c15e6381a8440a073a23980d7908650f8e76908f12124c07abba964ebf13af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
